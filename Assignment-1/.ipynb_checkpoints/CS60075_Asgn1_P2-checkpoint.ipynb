{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-sRzyQsUvBv"
   },
   "source": [
    "## CS60075 - Assignment - 1\n",
    "- Name : Debanjan Saha\n",
    "- Roll : 19CS30014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-6p-yxhVwnh"
   },
   "source": [
    "## Neural Network based Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T02:58:33.700860Z",
     "start_time": "2022-08-27T02:58:32.865095Z"
    },
    "id": "4STKCfy4bxYR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T02:58:34.421093Z",
     "start_time": "2022-08-27T02:58:34.301492Z"
    },
    "id": "wCYwXqn4Kjx2"
   },
   "outputs": [],
   "source": [
    "# directory for saving model\n",
    "!mkdir -p \"saved_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T02:58:36.215310Z",
     "start_time": "2022-08-27T02:58:36.210558Z"
    },
    "id": "zdN4HaL7WNCT"
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load dataset from the folder 'data'\n",
    "    \"\"\"\n",
    "    with open(\"./data/train.txt\", 'r') as f:\n",
    "        train_data = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    with open(\"./data/test.txt\", 'r') as f:\n",
    "        test_data = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T02:58:38.124963Z",
     "start_time": "2022-08-27T02:58:38.075764Z"
    },
    "id": "pvEVVlimWPGe"
   },
   "outputs": [],
   "source": [
    "train_sents, test_sents = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-27T02:58:38.556057Z",
     "start_time": "2022-08-27T02:58:38.544735Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2HfcF66djy8",
    "outputId": "f3175101-7499-4329-ef64-6f07b0456506"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['liberty all star usa sets initial payout',\n",
       " 'we are being accused of not implementing this agreement',\n",
       " 'entregrowth closed at 135 dlrs and options at 55 cents']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ejkFZwOdlhh",
    "outputId": "5793d582-484c-4ae6-8a40-eebb7e6dfa3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the company said each debenture is convertible into shares of businessland common stock at a conversion price of 2050 dlrs',\n",
       " 'sumita says he does not expect further dollar fall',\n",
       " 'the tin price is likely to rise to 20 ringgit a kilo this year because of the producers accord on export quotas and the reluctance of brokers and banks to sell the metal at lower prices a malaysian government bulletin said']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pYH1iSafjE3d"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters (Obtained after tuning them)\n",
    "hyperparams = {\n",
    "    \"embed_size\" : 128,\n",
    "    \"hidden_size\" : 1024,\n",
    "    \"num_layers\" : 1,\n",
    "    \"num_epochs\" : 5,\n",
    "    \"batch_size\" : 20,\n",
    "    \"seq_length\" : 30,\n",
    "    \"learning_rate\" : 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aQganrsB7ZZE"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "pyOvIyvUik8T"
   },
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    \"\"\"\n",
    "    Custom Dataset class to \n",
    "    - split train data into 90:10 train-dev split\n",
    "    - batchify train, test, dev data\n",
    "    - generate corpus using a word2vec model \n",
    "    \"\"\"\n",
    "    def __init__(self, train_sents, test_sents, batch_size=hyperparams[\"batch_size\"]):\n",
    "        self.train_sents = train_sents\n",
    "        self.test_sents = test_sents\n",
    "        self.batch_size = batch_size\n",
    "        self.generate_corpus()\n",
    "        \n",
    "    def generate_corpus(self):\n",
    "        \"\"\"\n",
    "        Generates the corpus by tokenizing the words in each sentence,\n",
    "        Makes a word2vec model on the tokens and generates a pretrained word2vec embedding for the corpus\n",
    "        \"\"\"\n",
    "\n",
    "        data = []\n",
    "        # iterate through each sentence in the training sentences\n",
    "        for sentence in self.train_sents:\n",
    "            cur_sent = []\n",
    "            # tokenize the sentence into words\n",
    "            for word in sentence.split():\n",
    "                cur_sent.append(word.lower())\n",
    "            data.append(cur_sent)\n",
    "\n",
    "        self.w2v_model = gensim.models.Word2Vec(data, min_count=1, vector_size=hyperparams[\"embed_size\"], window=5, workers=4)\n",
    "        self.word2idx = self.w2v_model.wv.key_to_index\n",
    "        self.idx2word = self.w2v_model.wv.index_to_key\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        \n",
    "        num_tokens = sum([len(words_array) for words_array in data])\n",
    "        self.ids = torch.LongTensor(num_tokens)\n",
    "        \n",
    "        index = 0\n",
    "        for sentence in data:\n",
    "            for word in sentence:\n",
    "                self.ids[index] = self.word2idx[word]\n",
    "                index += 1\n",
    "        \n",
    "    def create_batch(self, array, batch_size):\n",
    "        \"\"\"\n",
    "        creates batches of input array in given batch_size \n",
    "        \"\"\"\n",
    "        batched_total_size = (array.size(0) // batch_size) * batch_size \n",
    "        array = array[:batched_total_size]\n",
    "        return array.view(batch_size, -1)\n",
    "\n",
    "    def get_batched_train_and_dev_data(self, train_split=0.9):\n",
    "        \"\"\"\n",
    "        splits the train data into 90:10 ratio for train-dev set\n",
    "        batchifies both train and dev data for future use\n",
    "        \"\"\"\n",
    "        train_len = int(train_split * self.ids.size(0))\n",
    "        train_data, dev_data = self.ids[:train_len], self.ids[train_len:]\n",
    "        train_data, dev_data = self.create_batch(train_data, self.batch_size), self.create_batch(dev_data, self.batch_size)\n",
    "        return train_data, dev_data \n",
    "\n",
    "    def get_batched_test_data(self):\n",
    "        \"\"\"\n",
    "        generates test data tokens and batchifies the data for testing the model\n",
    "        \"\"\"\n",
    "        test_words = []\n",
    "        for sent in self.test_sents:\n",
    "            for word in sent.split():\n",
    "                if word in self.word2idx:\n",
    "                    test_words.append(word.lower())\n",
    "\n",
    "        test_data = torch.LongTensor(len(test_words))\n",
    "        index = 0\n",
    "        for word in test_words:\n",
    "            test_data[index] = self.word2idx[word]\n",
    "            index += 1\n",
    "\n",
    "        test_data = self.create_batch(test_data, self.batch_size)\n",
    "        return test_data\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        \"\"\"\n",
    "        returns vocab size for the dataset\n",
    "        \"\"\"\n",
    "        return self.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "y3kzQ6K9Blwe",
    "outputId": "a9f36562-c6be-401e-f865-a58bd1bec3fa"
   },
   "outputs": [],
   "source": [
    "dataset = CustomDataset(train_sents, test_sents, batch_size=hyperparams[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "id": "1YZVvsDICr9p",
    "outputId": "507c147b-d403-4adf-eda9-278780b8178f"
   },
   "outputs": [],
   "source": [
    "train_data, dev_data = dataset.get_batched_train_and_dev_data(train_split=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9a1aoJLENSI",
    "outputId": "a6084c47-ea4c-4349-dfaa-5927fe8a0560"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 60649]), torch.Size([20, 6738]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, dev_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ndD6mPsxE1wc"
   },
   "outputs": [],
   "source": [
    "test_data = dataset.get_batched_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YzGAtTslFP3X",
    "outputId": "f6977b6f-9af3-4b9c-bb3d-1a21c3d47e32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16507])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KVGXbXCFxHY",
    "outputId": "c27721f9-2a91-4202-d014-12e779958a3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44689"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8lB6IiQV8Cx",
    "outputId": "44d388ed-6603-429f-9858-87aedacfdb7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gp3K-qcqV8C2",
    "outputId": "cf340d13-cf9e-4542-87c3-e3ca6dd95f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44689\n"
     ]
    }
   ],
   "source": [
    "print(dataset.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "tftcXw1gJAUu"
   },
   "outputs": [],
   "source": [
    "pretrained_w2v_embeddings = torch.FloatTensor(dataset.w2v_model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "lw3dB8ifV8DB"
   },
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_layers, vocab_size):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(pretrained_w2v_embeddings)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Tjscc3EhLM38"
   },
   "outputs": [],
   "source": [
    "def detach(states):\n",
    "    return [state.detach() for state in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "7vn97dUhOscy"
   },
   "outputs": [],
   "source": [
    "model = RNNLanguageModel(\n",
    "    embed_size=hyperparams[\"embed_size\"], \n",
    "    hidden_size=hyperparams[\"hidden_size\"], \n",
    "    num_layers=hyperparams[\"num_layers\"],\n",
    "    vocab_size=dataset.get_vocab_size(),\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "DoOT72WEOzt8"
   },
   "outputs": [],
   "source": [
    "seq_length = hyperparams[\"seq_length\"]\n",
    "batch_size = hyperparams[\"batch_size\"]\n",
    "num_layers = hyperparams[\"num_layers\"]\n",
    "hidden_size = hyperparams[\"hidden_size\"]\n",
    "num_epochs = hyperparams[\"num_epochs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Mahq4bODK_xt",
    "outputId": "72585c5a-1a95-4a0f-fbec-89a75116f3e3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Running: Training Set\n",
      "Epoch [1/5], Step[0/2021], Loss: 10.7076, Perplexity: 44692.52\n",
      "Epoch [1/5], Step[100/2021], Loss: 7.1698, Perplexity: 1299.60\n",
      "Epoch [1/5], Step[200/2021], Loss: 6.1981, Perplexity: 491.84\n",
      "Epoch [1/5], Step[300/2021], Loss: 6.1978, Perplexity: 491.67\n",
      "Epoch [1/5], Step[400/2021], Loss: 5.9910, Perplexity: 399.81\n",
      "Epoch [1/5], Step[500/2021], Loss: 5.9888, Perplexity: 398.94\n",
      "Epoch [1/5], Step[600/2021], Loss: 5.6104, Perplexity: 273.24\n",
      "Epoch [1/5], Step[700/2021], Loss: 5.5426, Perplexity: 255.35\n",
      "Epoch [1/5], Step[800/2021], Loss: 5.3890, Perplexity: 218.99\n",
      "Epoch [1/5], Step[900/2021], Loss: 5.2950, Perplexity: 199.34\n",
      "Epoch [1/5], Step[1000/2021], Loss: 5.2092, Perplexity: 182.95\n",
      "Epoch [1/5], Step[1100/2021], Loss: 5.4622, Perplexity: 235.62\n",
      "Epoch [1/5], Step[1200/2021], Loss: 5.3439, Perplexity: 209.33\n",
      "Epoch [1/5], Step[1300/2021], Loss: 5.0160, Perplexity: 150.81\n",
      "Epoch [1/5], Step[1400/2021], Loss: 5.4980, Perplexity: 244.21\n",
      "Epoch [1/5], Step[1500/2021], Loss: 5.0080, Perplexity: 149.60\n",
      "Epoch [1/5], Step[1600/2021], Loss: 5.5104, Perplexity: 247.24\n",
      "Epoch [1/5], Step[1700/2021], Loss: 5.3384, Perplexity: 208.17\n",
      "Epoch [1/5], Step[1800/2021], Loss: 4.9951, Perplexity: 147.68\n",
      "Epoch [1/5], Step[1900/2021], Loss: 4.9631, Perplexity: 143.04\n",
      "Epoch [1/5], Step[2000/2021], Loss: 5.2638, Perplexity: 193.21\n",
      "Currently Running: Development Set\n",
      "Epoch [1/5], Step[0/224], Loss: 4.8558, Perplexity: 128.49\n",
      "Epoch [1/5], Step[100/224], Loss: 4.9857, Perplexity: 146.31\n",
      "Epoch [1/5], Step[200/224], Loss: 5.2533, Perplexity: 191.19\n",
      "Better Loss! Saving Model...\n",
      "Currently Running: Training Set\n",
      "Epoch [2/5], Step[0/2021], Loss: 5.3995, Perplexity: 221.29\n",
      "Epoch [2/5], Step[100/2021], Loss: 5.4201, Perplexity: 225.91\n",
      "Epoch [2/5], Step[200/2021], Loss: 4.8089, Perplexity: 122.59\n",
      "Epoch [2/5], Step[300/2021], Loss: 5.1999, Perplexity: 181.26\n",
      "Epoch [2/5], Step[400/2021], Loss: 4.9144, Perplexity: 136.24\n",
      "Epoch [2/5], Step[500/2021], Loss: 4.6800, Perplexity: 107.77\n",
      "Epoch [2/5], Step[600/2021], Loss: 4.6629, Perplexity: 105.94\n",
      "Epoch [2/5], Step[700/2021], Loss: 4.5971, Perplexity: 99.19\n",
      "Epoch [2/5], Step[800/2021], Loss: 4.6246, Perplexity: 101.96\n",
      "Epoch [2/5], Step[900/2021], Loss: 4.4461, Perplexity: 85.29\n",
      "Epoch [2/5], Step[1000/2021], Loss: 4.3652, Perplexity: 78.67\n",
      "Epoch [2/5], Step[1100/2021], Loss: 4.6010, Perplexity: 99.58\n",
      "Epoch [2/5], Step[1200/2021], Loss: 4.5583, Perplexity: 95.42\n",
      "Epoch [2/5], Step[1300/2021], Loss: 4.1832, Perplexity: 65.57\n",
      "Epoch [2/5], Step[1400/2021], Loss: 4.7139, Perplexity: 111.49\n",
      "Epoch [2/5], Step[1500/2021], Loss: 4.3248, Perplexity: 75.55\n",
      "Epoch [2/5], Step[1600/2021], Loss: 4.8310, Perplexity: 125.33\n",
      "Epoch [2/5], Step[1700/2021], Loss: 4.6004, Perplexity: 99.52\n",
      "Epoch [2/5], Step[1800/2021], Loss: 4.2166, Perplexity: 67.80\n",
      "Epoch [2/5], Step[1900/2021], Loss: 4.3026, Perplexity: 73.89\n",
      "Epoch [2/5], Step[2000/2021], Loss: 4.6045, Perplexity: 99.93\n",
      "Currently Running: Development Set\n",
      "Epoch [2/5], Step[0/224], Loss: 4.7987, Perplexity: 121.35\n",
      "Epoch [2/5], Step[100/224], Loss: 4.8085, Perplexity: 122.55\n",
      "Epoch [2/5], Step[200/224], Loss: 5.1189, Perplexity: 167.15\n",
      "Better Loss! Saving Model...\n",
      "Currently Running: Training Set\n",
      "Epoch [3/5], Step[0/2021], Loss: 4.7586, Perplexity: 116.59\n",
      "Epoch [3/5], Step[100/2021], Loss: 4.6502, Perplexity: 104.61\n",
      "Epoch [3/5], Step[200/2021], Loss: 4.1611, Perplexity: 64.14\n",
      "Epoch [3/5], Step[300/2021], Loss: 4.5585, Perplexity: 95.44\n",
      "Epoch [3/5], Step[400/2021], Loss: 4.3452, Perplexity: 77.11\n",
      "Epoch [3/5], Step[500/2021], Loss: 3.9003, Perplexity: 49.42\n",
      "Epoch [3/5], Step[600/2021], Loss: 4.0512, Perplexity: 57.46\n",
      "Epoch [3/5], Step[700/2021], Loss: 3.9242, Perplexity: 50.61\n",
      "Epoch [3/5], Step[800/2021], Loss: 4.0611, Perplexity: 58.04\n",
      "Epoch [3/5], Step[900/2021], Loss: 3.8522, Perplexity: 47.10\n",
      "Epoch [3/5], Step[1000/2021], Loss: 3.7556, Perplexity: 42.76\n",
      "Epoch [3/5], Step[1100/2021], Loss: 3.9703, Perplexity: 53.00\n",
      "Epoch [3/5], Step[1200/2021], Loss: 3.9411, Perplexity: 51.47\n",
      "Epoch [3/5], Step[1300/2021], Loss: 3.6283, Perplexity: 37.65\n",
      "Epoch [3/5], Step[1400/2021], Loss: 4.0629, Perplexity: 58.14\n",
      "Epoch [3/5], Step[1500/2021], Loss: 3.7811, Perplexity: 43.87\n",
      "Epoch [3/5], Step[1600/2021], Loss: 4.2235, Perplexity: 68.27\n",
      "Epoch [3/5], Step[1700/2021], Loss: 4.0029, Perplexity: 54.76\n",
      "Epoch [3/5], Step[1800/2021], Loss: 3.6183, Perplexity: 37.27\n",
      "Epoch [3/5], Step[1900/2021], Loss: 3.7301, Perplexity: 41.68\n",
      "Epoch [3/5], Step[2000/2021], Loss: 4.0200, Perplexity: 55.70\n",
      "Currently Running: Development Set\n",
      "Epoch [3/5], Step[0/224], Loss: 4.8796, Perplexity: 131.57\n",
      "Epoch [3/5], Step[100/224], Loss: 4.8270, Perplexity: 124.84\n",
      "Epoch [3/5], Step[200/224], Loss: 5.1723, Perplexity: 176.32\n",
      "Currently Running: Training Set\n",
      "Epoch [4/5], Step[0/2021], Loss: 4.1768, Perplexity: 65.16\n",
      "Epoch [4/5], Step[100/2021], Loss: 3.9580, Perplexity: 52.35\n",
      "Epoch [4/5], Step[200/2021], Loss: 3.6603, Perplexity: 38.87\n",
      "Epoch [4/5], Step[300/2021], Loss: 4.0435, Perplexity: 57.03\n",
      "Epoch [4/5], Step[400/2021], Loss: 3.8806, Perplexity: 48.45\n",
      "Epoch [4/5], Step[500/2021], Loss: 3.3179, Perplexity: 27.60\n",
      "Epoch [4/5], Step[600/2021], Loss: 3.5528, Perplexity: 34.91\n",
      "Epoch [4/5], Step[700/2021], Loss: 3.3794, Perplexity: 29.35\n",
      "Epoch [4/5], Step[800/2021], Loss: 3.6359, Perplexity: 37.94\n",
      "Epoch [4/5], Step[900/2021], Loss: 3.3528, Perplexity: 28.58\n",
      "Epoch [4/5], Step[1000/2021], Loss: 3.2626, Perplexity: 26.12\n",
      "Epoch [4/5], Step[1100/2021], Loss: 3.4983, Perplexity: 33.06\n",
      "Epoch [4/5], Step[1200/2021], Loss: 3.4557, Perplexity: 31.68\n",
      "Epoch [4/5], Step[1300/2021], Loss: 3.2041, Perplexity: 24.63\n",
      "Epoch [4/5], Step[1400/2021], Loss: 3.6128, Perplexity: 37.07\n",
      "Epoch [4/5], Step[1500/2021], Loss: 3.3474, Perplexity: 28.43\n",
      "Epoch [4/5], Step[1600/2021], Loss: 3.7660, Perplexity: 43.21\n",
      "Epoch [4/5], Step[1700/2021], Loss: 3.5959, Perplexity: 36.45\n",
      "Epoch [4/5], Step[1800/2021], Loss: 3.2063, Perplexity: 24.69\n",
      "Epoch [4/5], Step[1900/2021], Loss: 3.2665, Perplexity: 26.22\n",
      "Epoch [4/5], Step[2000/2021], Loss: 3.5334, Perplexity: 34.24\n",
      "Currently Running: Development Set\n",
      "Epoch [4/5], Step[0/224], Loss: 4.9854, Perplexity: 146.27\n",
      "Epoch [4/5], Step[100/224], Loss: 4.9297, Perplexity: 138.33\n",
      "Epoch [4/5], Step[200/224], Loss: 5.3281, Perplexity: 206.05\n",
      "Currently Running: Training Set\n",
      "Epoch [5/5], Step[0/2021], Loss: 3.6822, Perplexity: 39.73\n",
      "Epoch [5/5], Step[100/2021], Loss: 3.4707, Perplexity: 32.16\n",
      "Epoch [5/5], Step[200/2021], Loss: 3.2822, Perplexity: 26.63\n",
      "Epoch [5/5], Step[300/2021], Loss: 3.6406, Perplexity: 38.11\n",
      "Epoch [5/5], Step[400/2021], Loss: 3.5175, Perplexity: 33.70\n",
      "Epoch [5/5], Step[500/2021], Loss: 2.9320, Perplexity: 18.76\n",
      "Epoch [5/5], Step[600/2021], Loss: 3.1751, Perplexity: 23.93\n",
      "Epoch [5/5], Step[700/2021], Loss: 3.0326, Perplexity: 20.75\n",
      "Epoch [5/5], Step[800/2021], Loss: 3.3193, Perplexity: 27.64\n",
      "Epoch [5/5], Step[900/2021], Loss: 2.9912, Perplexity: 19.91\n",
      "Epoch [5/5], Step[1000/2021], Loss: 2.9772, Perplexity: 19.63\n",
      "Epoch [5/5], Step[1100/2021], Loss: 3.1334, Perplexity: 22.95\n",
      "Epoch [5/5], Step[1200/2021], Loss: 3.1240, Perplexity: 22.74\n",
      "Epoch [5/5], Step[1300/2021], Loss: 2.8862, Perplexity: 17.93\n",
      "Epoch [5/5], Step[1400/2021], Loss: 3.3020, Perplexity: 27.17\n",
      "Epoch [5/5], Step[1500/2021], Loss: 3.0503, Perplexity: 21.12\n",
      "Epoch [5/5], Step[1600/2021], Loss: 3.4539, Perplexity: 31.62\n",
      "Epoch [5/5], Step[1700/2021], Loss: 3.2559, Perplexity: 25.94\n",
      "Epoch [5/5], Step[1800/2021], Loss: 2.8770, Perplexity: 17.76\n",
      "Epoch [5/5], Step[1900/2021], Loss: 2.9542, Perplexity: 19.19\n",
      "Epoch [5/5], Step[2000/2021], Loss: 3.1960, Perplexity: 24.43\n",
      "Currently Running: Development Set\n",
      "Epoch [5/5], Step[0/224], Loss: 5.1126, Perplexity: 166.09\n",
      "Epoch [5/5], Step[100/224], Loss: 5.0048, Perplexity: 149.12\n",
      "Epoch [5/5], Step[200/224], Loss: 5.4130, Perplexity: 224.31\n"
     ]
    }
   ],
   "source": [
    "best_loss = math.inf\n",
    "\n",
    "num_train_batches = train_data.size(1) // seq_length\n",
    "num_dev_batches = dev_data.size(1) // seq_length\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "    \n",
    "\n",
    "    print(\"Currently Running: Training Set\")\n",
    "    model.train()\n",
    "    for i in range(0, train_data.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = train_data[:, i:i+seq_length].to(device)\n",
    "        targets = train_data[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        states = detach(states)\n",
    "        outputs, states = model(inputs, states)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(epoch+1, num_epochs, step, num_train_batches, loss.item(), np.exp(loss.item())))\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "    \n",
    "    print(\"Currently Running: Development Set\")\n",
    "    model.eval()\n",
    "    for i in range(0, dev_data.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = dev_data[:, i:i+seq_length].to(device)\n",
    "        targets = dev_data[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        states = detach(states)\n",
    "        outputs, states = model(inputs, states)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                    .format(epoch+1, num_epochs, step, num_dev_batches, loss.item(), np.exp(loss.item())))\n",
    "\n",
    "    # print(f\"Total Loss: {total_loss}, best_loss: {best_loss}\")\n",
    "\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        print(\"Better Loss! Saving Model...\")\n",
    "        # Save the model checkpoints\n",
    "        torch.save(model.state_dict(), 'saved_model/rnn_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "b7p8hU1aV8DE"
   },
   "outputs": [],
   "source": [
    "# del model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "RwQF4knAbSL9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLanguageModel(\n",
       "  (embed): Embedding(44689, 128)\n",
       "  (lstm): LSTM(128, 1024, batch_first=True)\n",
       "  (linear): Linear(in_features=1024, out_features=44689, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"saved_model/rnn_model.ckpt\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "0-vkUN64V8DE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step[0/550], Perplexity: 226.14\n",
      "Step[100/550], Perplexity: 132.66\n",
      "Step[200/550], Perplexity: 190.19\n",
      "Step[300/550], Perplexity: 148.76\n",
      "Step[400/550], Perplexity: 166.58\n",
      "Step[500/550], Perplexity: 130.81\n",
      "Test Set Perplexity: 154.42289450497304\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    total_perplexity = 0.0\n",
    "    sequence_counter = 0\n",
    "\n",
    "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "            torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "\n",
    "    num_test_batches = test_data.size(1) // seq_length\n",
    "\n",
    "    model.eval()\n",
    "    for i in range(0, test_data.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = test_data[:, i:i+seq_length].to(device)\n",
    "        targets = test_data[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        states = detach(states)\n",
    "        outputs, states = model(inputs, states)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "\n",
    "        current_seq_perplexity = np.exp(loss.item())\n",
    "        total_perplexity += current_seq_perplexity\n",
    "        sequence_counter += 1\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Step[{}/{}], Perplexity: {:5.2f}'.format(step, num_test_batches, current_seq_perplexity))\n",
    "    \n",
    "    print(f\"Test Set Perplexity: {total_perplexity / sequence_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJRGjBLAT4E3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS60075-Asgn1-P2.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
